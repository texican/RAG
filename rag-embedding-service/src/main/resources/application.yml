server:
  port: 8083

spring:
  application:
    name: rag-embedding-service
  
  # Redis Configuration for Vector Storage
  data:
    redis:
      host: localhost
      port: 6379
      database: 0  # Use DB 0 with key prefix byo_rag_local:embedding
      timeout: 2000ms
      lettuce:
        pool:
          max-active: 10
          max-idle: 8
          min-idle: 2
          max-wait: -1ms
  
  # Kafka Configuration
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: embedding-service
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      properties:
        spring.json.trusted.packages: "com.byo.rag.*"
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      acks: 1
      retries: 3
      batch-size: 16384
      linger-ms: 1
      buffer-memory: 33554432

# Spring AI Configuration
spring.ai:
  openai:
    api-key: ${OPENAI_API_KEY:your-openai-api-key}
    embedding:
      options:
        model: text-embedding-3-small
        dimensions: 1536
  
  transformers:
    onnx:
      model:
        path: ${TRANSFORMERS_MODEL_PATH:models/sentence-transformers/all-MiniLM-L6-v2}

# Application Configuration
embedding:
  # Vector Storage Configuration
  vector:
    redis:
      index-prefix: "byo_rag_local:embedding:vectors"
      batch-size: 100
      dimension: 1536
      similarity-algorithm: COSINE
      ef-construction: 200
      ef-runtime: 10
      max-connections: 16
  
  # Model Configuration
  models:
    default: openai-text-embedding-3-small
    fallback: sentence-transformers-all-minilm-l6-v2
    cache-ttl: 3600  # Cache embeddings for 1 hour
    
  # Batch Processing Configuration
  batch:
    size: 50
    timeout: 30s
    max-retries: 3
    parallel-processing: true
    max-threads: 4

# Kafka Topics
kafka:
  topics:
    embedding-generation: embedding-generation
    embedding-complete: embedding-complete
    vector-search: vector-search
    dead-letter-queue: embedding-dlq
    failure-alerts: failure-alerts

# Resilience4j Configuration
resilience4j:
  circuitbreaker:
    instances:
      embeddingService:
        register-health-indicator: true
        event-consumer-buffer-size: 10
        failure-rate-threshold: 50
        minimum-number-of-calls: 5
        automatic-transition-from-open-to-half-open-enabled: true
        wait-duration-in-open-state: 30s
        permitted-number-of-calls-in-half-open-state: 3
        sliding-window-size: 20
        sliding-window-type: COUNT_BASED
      kafka:
        register-health-indicator: true
        event-consumer-buffer-size: 10
        failure-rate-threshold: 60
        minimum-number-of-calls: 3
        automatic-transition-from-open-to-half-open-enabled: true
        wait-duration-in-open-state: 15s
        permitted-number-of-calls-in-half-open-state: 2
        sliding-window-size: 10
        sliding-window-type: COUNT_BASED
  retry:
    instances:
      embeddingGeneration:
        max-attempts: 3
        wait-duration: 1s
        exponential-backoff-multiplier: 2
        retry-exceptions:
          - java.lang.RuntimeException
          - org.springframework.ai.retry.TransientAiException
        ignore-exceptions:
          - java.lang.IllegalArgumentException

# Monitoring Configuration
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
  endpoint:
    health:
      show-details: when-authorized
  prometheus:
    metrics:
      export:
        enabled: true
  info:
    env:
      enabled: true
    java:
      enabled: true

# Logging Configuration
logging:
  level:
    com.byo.rag: DEBUG
    org.springframework.ai: INFO
    org.springframework.kafka: WARN
    redis.clients.jedis: WARN
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level [%X{traceId:-},%X{spanId:-}] %logger{36} - %msg%n"
    file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level [%X{traceId:-},%X{spanId:-}] %logger{36} - %msg%n"
  file:
    name: logs/embedding-service.log
  logback:
    rollingpolicy:
      max-file-size: 10MB
      max-history: 30

# Custom Application Properties
app:
  name: RAG Embedding Service
  version: 1.0.0
  description: Vector operations and similarity search service for enterprise RAG system

---
# Docker Profile
spring:
  config:
    activate:
      on-profile: docker

  data:
    redis:
      host: ${REDIS_HOST:redis}
      port: ${REDIS_PORT:6379}
      password: ${REDIS_PASSWORD}
      database: 0
      timeout: 2000ms
      lettuce:
        pool:
          max-active: 10
          max-idle: 8
          min-idle: 2
          max-wait: -1ms

  # Disable Kafka in Docker for now (will be enabled later)
  autoconfigure:
    exclude: org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration

# Spring AI Configuration for Docker
spring.ai:
  ollama:
    base-url: ${OLLAMA_URL:http://ollama:11434}

# Ollama Embedding Model Configuration
embedding:
  models:
    ollama: mxbai-embed-large
  # Override vector dimensions for Ollama embeddings (1024 vs 1536 for OpenAI)
  vector:
    redis:
      dimension: 1024

logging:
  level:
    com.byo.rag: INFO
    org.springframework.ai: INFO