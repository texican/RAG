server:
  port: 8083

spring:
  application:
    name: rag-embedding-service
  
  # Redis Configuration for Vector Storage
  data:
    redis:
      host: localhost
      port: 6379
      database: 2  # Use separate database for embeddings
      timeout: 2000ms
      lettuce:
        pool:
          max-active: 10
          max-idle: 8
          min-idle: 2
          max-wait: -1ms
  
  # Kafka Configuration
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: embedding-service
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      properties:
        spring.json.trusted.packages: "com.enterprise.rag.*"
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      acks: 1
      retries: 3
      batch-size: 16384
      linger-ms: 1
      buffer-memory: 33554432

# Spring AI Configuration
spring.ai:
  openai:
    api-key: ${OPENAI_API_KEY:your-openai-api-key}
    embedding:
      options:
        model: text-embedding-3-small
        dimensions: 1536
  
  transformers:
    onnx:
      model:
        path: ${TRANSFORMERS_MODEL_PATH:models/sentence-transformers/all-MiniLM-L6-v2}

# Application Configuration
embedding:
  # Vector Storage Configuration
  vector:
    redis:
      index-prefix: "rag:vectors"
      batch-size: 100
      dimension: 1536
      similarity-algorithm: COSINE
      ef-construction: 200
      ef-runtime: 10
      max-connections: 16
  
  # Model Configuration
  models:
    default: openai-text-embedding-3-small
    fallback: sentence-transformers-all-minilm-l6-v2
    cache-ttl: 3600  # Cache embeddings for 1 hour
    
  # Batch Processing Configuration
  batch:
    size: 50
    timeout: 30s
    max-retries: 3
    parallel-processing: true
    max-threads: 4

# Kafka Topics
kafka:
  topics:
    embedding-generation: embedding-generation
    embedding-complete: embedding-complete
    vector-search: vector-search

# Monitoring Configuration
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
  endpoint:
    health:
      show-details: when-authorized
  metrics:
    export:
      prometheus:
        enabled: true
  info:
    env:
      enabled: true
    java:
      enabled: true

# Logging Configuration
logging:
  level:
    com.enterprise.rag: DEBUG
    org.springframework.ai: INFO
    org.springframework.kafka: WARN
    redis.clients.jedis: WARN
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level [%X{traceId:-},%X{spanId:-}] %logger{36} - %msg%n"
    file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level [%X{traceId:-},%X{spanId:-}] %logger{36} - %msg%n"
  file:
    name: logs/embedding-service.log
  logback:
    rollingpolicy:
      max-file-size: 10MB
      max-history: 30

# Custom Application Properties
app:
  name: RAG Embedding Service
  version: 1.0.0
  description: Vector operations and similarity search service for enterprise RAG system

---
# Docker Profile
spring:
  config:
    activate:
      on-profile: docker

  data:
    redis:
      host: ${REDIS_HOST:redis}
      port: ${REDIS_PORT:6379}
      password: ${REDIS_PASSWORD}
      database: 2
      timeout: 2000ms
      lettuce:
        pool:
          max-active: 10
          max-idle: 8
          min-idle: 2
          max-wait: -1ms

  # Disable Kafka in Docker for now (will be enabled later)
  autoconfigure:
    exclude: org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration

# Spring AI Configuration for Docker
spring.ai:
  ollama:
    base-url: ${OLLAMA_URL:http://ollama:11434}

logging:
  level:
    com.enterprise.rag: INFO
    org.springframework.ai: INFO