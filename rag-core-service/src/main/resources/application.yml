server:
  port: 8084

spring:
  application:
    name: rag-core-service
  
  # Database Configuration
  datasource:
    url: jdbc:postgresql://localhost:5432/rag_db
    username: rag_user
    password: rag_password
    driver-class-name: org.postgresql.Driver
    hikari:
      maximum-pool-size: 15
      minimum-idle: 5
      connection-timeout: 20000
      idle-timeout: 300000
      max-lifetime: 1200000
  
  jpa:
    hibernate:
      ddl-auto: validate
    show-sql: false
    properties:
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQLDialect
        format_sql: false
        generate_statistics: false
        cache:
          use_second_level_cache: false
          use_query_cache: false
        jdbc:
          batch_size: 25
          order_inserts: true
          order_updates: true
          batch_versioned_data: true
  
  # Redis Configuration for Caching
  data:
    redis:
      host: localhost
      port: 6379
      database: 1  # Use separate database for core service cache
      timeout: 2000ms
      lettuce:
        pool:
          max-active: 15
          max-idle: 10
          min-idle: 3
          max-wait: -1ms

# Redis Jedis Configuration (for VectorSearchService)
redis:
  jedis:
    timeout: 2000  # milliseconds, no suffix
  
  # Kafka Configuration
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: core-service
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      properties:
        spring.json.trusted.packages: "com.byo.rag.*"
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      acks: 1
      retries: 3
      batch-size: 16384
      linger-ms: 1
      buffer-memory: 33554432

# Spring AI Configuration  
spring.ai:
  ollama:
    base-url: ${OLLAMA_BASE_URL:http://localhost:11434}
    chat:
      enabled: ${OLLAMA_ENABLED:false}
      options:
        model: llama3.1:8b
        temperature: 0.1
        num-predict: 2000
        top-p: 0.9
        repeat-penalty: 1.1
        
  openai:
    api-key: ${OPENAI_API_KEY:demo-key}
    chat:
      enabled: ${OPENAI_ENABLED:true}
      options:
        model: gpt-4o-mini
        temperature: 0.1
        max-tokens: 2000
        top-p: 0.9
        frequency-penalty: 0.0
        presence-penalty: 0.0
        
  anthropic:
    api-key: ${ANTHROPIC_API_KEY:demo-key}
    chat:
      enabled: ${ANTHROPIC_ENABLED:false}
      options:
        model: claude-3-haiku-20240307
        temperature: 0.1
        max-tokens: 2000
        top-p: 0.9

# RAG Configuration
rag:
  # Context Assembly Configuration
  context:
    max-tokens: 4000
    chunk-separator: "\n\n---\n\n"
    include-metadata: true
    relevance-threshold: 0.7
    
  # Query Optimization Configuration
  query:
    optimization:
      enabled: true
      min-length: 3
      max-length: 500
      expand-acronyms: true
      remove-stopwords: false
    
  # Response Generation Configuration  
  generation:
    default-provider: openai
    fallback-provider: anthropic
    tertiary-provider: ollama
    max-retries: 3
    timeout: 30s
    streaming: true

# LLM Configuration
llm:
  default-provider: openai
  fallback-provider: ollama
  max-tokens: 1500
  temperature: 0.7
  timeout-seconds: 30

# Cache Configuration
cache:
  response:
    enabled: true
    ttl-minutes: 60
  embedding:
    enabled: true
    ttl-hours: 24
  similarity-threshold: 0.95
  key-prefix: rag-cache

# Conversation Configuration
conversation:
  max-history: 20
  context-window: 5
  ttl-hours: 24
  enable-context: true

# Service URLs for Feign clients
services:
  embedding:
    url: ${EMBEDDING_SERVICE_URL:http://localhost:8084}
    timeout: 30s
    retry-attempts: 3
  
  document:
    url: ${DOCUMENT_SERVICE_URL:http://localhost:8083}
    timeout: 15s
    retry-attempts: 2

# Kafka Topics
kafka:
  topics:
    rag-queries: rag-queries
    rag-responses: rag-responses
    feedback: feedback

# Monitoring Configuration
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus,rag
  endpoint:
    health:
      show-details: when-authorized
    rag:
      enabled: true
  metrics:
    export:
      prometheus:
        enabled: true
    tags:
      service: rag-core-service
      version: 1.0.0
  info:
    env:
      enabled: true
    java:
      enabled: true
    build:
      enabled: true

# Logging Configuration
logging:
  level:
    com.byo.rag: DEBUG
    org.springframework.ai: INFO
    org.springframework.kafka: WARN
    dev.langchain4j: INFO
    org.springframework.cloud.openfeign: DEBUG
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level [%X{traceId:-},%X{spanId:-}] %logger{36} - %msg%n"
    file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level [%X{traceId:-},%X{spanId:-}] %logger{36} - %msg%n"
  file:
    name: logs/core-service.log
  logback:
    rollingpolicy:
      max-file-size: 10MB
      max-history: 30

# Custom Application Properties
app:
  name: RAG Core Service
  version: 1.0.0
  description: RAG query engine and LLM integration service

---
spring:
  config:
    activate:
      on-profile: docker

  datasource:
    url: jdbc:postgresql://${DB_HOST:postgres}:5432/${DB_NAME:rag_enterprise}
    username: ${DB_USERNAME:rag_user}
    password: ${DB_PASSWORD:rag_password}

  data:
    redis:
      host: ${REDIS_HOST:redis}
      password: ${REDIS_PASSWORD:redis_password}

  kafka:
    bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:kafka:9092}